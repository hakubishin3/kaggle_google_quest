{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import random\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tqdm(it, *args, **kwargs):\n",
    "    return it\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n",
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "PATH = '../input/google-quest-challenge/'\n",
    "\n",
    "BERT_PATH = '../input/bertpretrained/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.question_body = df_train.question_body.apply(html.unescape)\n",
    "df_train.question_title = df_train.question_title.apply(html.unescape)\n",
    "df_train.answer = df_train.answer.apply(html.unescape)\n",
    "\n",
    "df_test.question_body = df_test.question_body.apply(html.unescape)\n",
    "df_test.question_title = df_test.question_title.apply(html.unescape)\n",
    "df_test.answer = df_test.answer.apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2054, 2572, 1045, 3974, 2043, 2478, 5331, 10868, 2612, 1997, 1037, 26632, 10014, 1029, 102, 2044, 2652, 2105, 2007, 26632, 5855, 2006, 1011, 1996, 1011, 10036, 1006, 3191, 1024, 11674, 10014, 1010, 7065, 1012, 10014, 5614, 2006, 1037, 3442, 10014, 1010, 13135, 5331, 10868, 1007, 1010, 1045, 2052, 2066, 2000, 2131, 2582, 2007, 2023, 1012, 1996, 3471, 2007, 1996, 5461, 1045, 2109, 2003, 2008, 3579, 2003, 6410, 1998, 18892, 2491, 2003, 18636, 2012, 2190, 1012, 2023, 3132, 2026, 16437, 2000, 2145, 5739, 1006, 3191, 1024, 2757, 9728, 1007, 2085, 1010, 2004, 3500, 2003, 8455, 1010, 1045, 2215, 2000, 2022, 2583, 2000, 5607, 2444, 9728, 1012, 1045, 2903, 2008, 2005, 2023, 1010, 8285, 14876, 7874, 1998, 2275, 10880, 18892, 2097, 2022, 1997, 2307, 2393, 1012, 2061, 1010, 2028, 5793, 2021, 6450, 5724, 2003, 1037, 26632, 10014, 1006, 2360, 1010, 1041, 2546, 2531, 7382, 26632, 1007, 2174, 1010, 1045, 2572, 2025, 2428, 4699, 1999, 2664, 2178, 3539, 10014, 1012, 2019, 4522, 2003, 1996, 5992, 5331, 10868, 1012, 3272, 2005, 4555, 7995, 3292, 1010, 2054, 2572, 1045, 3974, 2043, 2478, 10868, 1006, 11211, 2007, 1037, 2986, 10014, 1010, 2360, 1041, 2546, 19841, 1011, 3263, 1013, 1016, 1012, 1022, 1007, 2612, 1997, 1037, 26632, 10014, 1029, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "['[CLS]', 'what', 'am', 'i', 'losing', 'when', 'using', 'extension', 'tubes', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', 'after', 'playing', 'around', 'with', 'macro', 'photography', 'on', '-', 'the', '-', 'cheap', '(', 'read', ':', 'reversed', 'lens', ',', 'rev', '.', 'lens', 'mounted', 'on', 'a', 'straight', 'lens', ',', 'passive', 'extension', 'tubes', ')', ',', 'i', 'would', 'like', 'to', 'get', 'further', 'with', 'this', '.', 'the', 'problems', 'with', 'the', 'techniques', 'i', 'used', 'is', 'that', 'focus', 'is', 'manual', 'and', 'aperture', 'control', 'is', 'problematic', 'at', 'best', '.', 'this', 'limited', 'my', 'setup', 'to', 'still', 'subjects', '(', 'read', ':', 'dead', 'insects', ')', 'now', ',', 'as', 'spring', 'is', 'approaching', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'shoot', 'live', 'insects', '.', 'i', 'believe', 'that', 'for', 'this', ',', 'auto', '##fo', '##cus', 'and', 'set', '##table', 'aperture', 'will', 'be', 'of', 'great', 'help', '.', 'so', ',', 'one', 'obvious', 'but', 'expensive', 'option', 'is', 'a', 'macro', 'lens', '(', 'say', ',', 'e', '##f', '100', '##mm', 'macro', ')', 'however', ',', 'i', 'am', 'not', 'really', 'interested', 'in', 'yet', 'another', 'prime', 'lens', '.', 'an', 'alternative', 'is', 'the', 'electrical', 'extension', 'tubes', '.', 'except', 'for', 'maximum', 'focusing', 'distance', ',', 'what', 'am', 'i', 'losing', 'when', 'using', 'tubes', '(', 'coupled', 'with', 'a', 'fine', 'lens', ',', 'say', 'e', '##f', '##70', '-', '200', '/', '2', '.', '8', ')', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "def _preprocess_text(s: str) -> str:\n",
    "    return s\n",
    "\n",
    "\n",
    "def _trim_input(question_tokens: List[str], answer_tokens: List[str], max_sequence_length: int, q_max_len: int, a_max_len: int) -> Tuple[List[str], List[str]]:\n",
    "    q_len = len(question_tokens)\n",
    "    a_len = len(answer_tokens)\n",
    "    if q_len + a_len + 3 > max_sequence_length:\n",
    "        if a_max_len <= a_len and q_max_len <= q_len:\n",
    "            ## Answer も Question も長過ぎる場合、どちらも限界まで切り詰めるしかない\n",
    "            q_new_len_head = floor((q_max_len - q_max_len/2))\n",
    "            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n",
    "            a_new_len_head = floor((a_max_len - a_max_len/2))\n",
    "            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n",
    "        elif q_len <= a_len and q_len < q_max_len:\n",
    "            ## Answer のほうが長く、Question が十分短いなら、その分 Answer にまわす\n",
    "            a_max_len = a_max_len + (q_max_len - q_len - 1)\n",
    "            a_new_len_head = floor((a_max_len - a_max_len/2))\n",
    "            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n",
    "        elif a_len < q_len:\n",
    "            assert a_len <= a_max_len\n",
    "            q_max_len = q_max_len + (a_max_len - a_len - 1)\n",
    "            q_new_len_head = floor((q_max_len - q_max_len/2))\n",
    "            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n",
    "        else:\n",
    "            raise ValueError(\"unreachable: q_len: {}, a_len: {}, q_max_len: {}, a_max_len: {}\".format(q_len, a_len, q_max_len, a_max_len))\n",
    "    return question_tokens, answer_tokens\n",
    "\n",
    "\n",
    "def _convert_to_transformer_inputs(title: str, question: str, answer: str, tokenizer: BertTokenizer, question_only=False):\n",
    "    title = _preprocess_text(title)\n",
    "    question = _preprocess_text(question)\n",
    "    answer = _preprocess_text(answer)\n",
    "    question = \"{} [SEP] {}\".format(title, question)\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    if question_only:\n",
    "        answer_tokens = []\n",
    "    else:\n",
    "        answer_tokens = tokenizer.tokenize(answer)\n",
    "    question_tokens, answer_tokens = _trim_input(question_tokens, answer_tokens, MAX_SEQUENCE_LENGTH, (MAX_SEQUENCE_LENGTH - 3) // 2, (MAX_SEQUENCE_LENGTH - 3) // 2)\n",
    "    ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + question_tokens + [\"[SEP]\"] + answer_tokens + [\"[SEP]\"])\n",
    "    padded_ids = ids + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
    "    token_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
    "    attention_mask = [1] * len(ids) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n",
    "    return padded_ids, token_type_ids, attention_mask\n",
    "\n",
    "sample_args = df_train[\"question_title\"].values[0], df_train[\"question_body\"].values[0], df_train[\"answer\"].values[0]\n",
    "sample_ids = _convert_to_transformer_inputs(*sample_args, tokenizer, question_only=True)\n",
    "print(sample_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(sample_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, question_only=False):\n",
    "    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n",
    "    for title, body, answer in zip(df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values):\n",
    "        ids, type_ids, mask = _convert_to_transformer_inputs(title, body, answer, tokenizer, question_only=question_only)\n",
    "        input_ids.append(ids)\n",
    "        input_token_type_ids.append(type_ids)\n",
    "        input_attention_masks.append(mask)\n",
    "    return (\n",
    "        np.asarray(input_ids, dtype=np.int32),\n",
    "        np.asarray(input_token_type_ids, dtype=np.int32),\n",
    "        np.asarray(input_attention_masks, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_output_arrays(df):\n",
    "    return np.asarray(df[output_categories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_json_file(BERT_PATH + \"/bert_config.json\")\n",
    "        config.output_hidden_states = True\n",
    "        self.bert = BertForPreTraining.from_pretrained(BERT_PATH + \"/bert_model.ckpt.index\", from_tf=True, config=config).bert\n",
    "        self.cls_token_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 4, 768),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.qa_sep_token_head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 4, 768),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 2, 30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_mask, -1) - 1)\n",
    "\n",
    "#         p_question_answer_dropout = 0.2\n",
    "#         if self.training and random.random() < p_question_answer_dropout:\n",
    "#             if random.random() < 0.5:\n",
    "#                 # mask question\n",
    "#                 attention_mask = attention_mask * (token_type_ids == 1)\n",
    "#             else:\n",
    "#                 # mask answer\n",
    "#                 attention_mask = attention_mask * (token_type_ids == 0)\n",
    "        \n",
    "        _, _, hidden_states = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n",
    "        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n",
    "        x_cls = self.cls_token_head(x)\n",
    "        \n",
    "        # Gather [SEP] hidden states\n",
    "        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n",
    "        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n",
    "        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n",
    "        \n",
    "        x_qa_sep = self.qa_sep_token_head(x)\n",
    "        x = torch.cat([x_cls, x_qa_sep], -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.tensor(compute_output_arrays(df_train), dtype=torch.float)\n",
    "inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train)]\n",
    "question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train, question_only=True)]\n",
    "test_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test)]\n",
    "test_question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test, question_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "cls_token_head.1.weight\n",
      "cls_token_head.1.bias\n",
      "qa_sep_token_head.1.weight\n",
      "qa_sep_token_head.1.bias\n",
      "linear.1.weight\n",
      "linear.1.bias\n"
     ]
    }
   ],
   "source": [
    "for n, _ in Model().named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding \t 0.9666648\n",
      "question_body_critical \t 0.58160704\n",
      "question_conversational \t 0.7005903\n",
      "question_expect_short_answer \t 0.36372498\n",
      "question_fact_seeking \t 0.4212381\n",
      "question_has_commonly_accepted_answer \t 0.3791943\n",
      "question_interestingness_others \t 0.9392594\n",
      "question_interestingness_self \t 0.6863097\n",
      "question_multi_intent \t 0.3809655\n",
      "question_not_really_a_question \t 2.7880754\n",
      "question_opinion_seeking \t 0.34880248\n",
      "question_type_choice \t 0.346085\n",
      "question_type_compare \t 0.8308305\n",
      "question_type_consequence \t 1.7193657\n",
      "question_type_definition \t 0.92452765\n",
      "question_type_entity \t 0.646035\n",
      "question_type_instructions \t 0.3016625\n",
      "question_type_procedure \t 0.4960922\n",
      "question_type_reason_explanation \t 0.33294326\n",
      "question_type_spelling \t 6.230046\n",
      "question_well_written \t 0.7154175\n",
      "answer_helpful \t 1.1115448\n",
      "answer_level_of_information \t 1.1855657\n",
      "answer_plausible \t 1.4684314\n",
      "answer_relevance \t 1.7103598\n",
      "answer_satisfaction \t 0.97630584\n",
      "answer_type_instructions \t 0.3018177\n",
      "answer_type_procedure \t 0.56550634\n",
      "answer_type_reason_explanation \t 0.3135491\n",
      "answer_well_written \t 1.2674823\n"
     ]
    }
   ],
   "source": [
    "LABEL_WEIGHTS = torch.tensor(1.0 / df_train[output_categories].std().values, dtype=torch.float32).to(device)\n",
    "LABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\n",
    "for name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n",
    "    print(name, \"\\t\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_BINS = [400, 400, 15, 100, 400, 7, 1600, 100, 100, 400, 100, 9, 8, 50, 9, 8, 15, 400, 400, 5, 400, 400, 800, 50, 200, 1600, 20, 200, 1600, 1600]\n",
    "\n",
    "def binning_output(preds, n_bins=BEST_BINS):\n",
    "    preds = preds.copy()\n",
    "    for i in range(preds.shape[-1]):\n",
    "        n = n_bins[i]\n",
    "        binned = (preds[:, i] * n).astype(np.int32).astype(np.float32) / n\n",
    "        unique_values, unique_counts = np.unique(binned, return_counts=True)\n",
    "        # 多数派以外が 0.5 % を下回ったら binning をやめる\n",
    "        minor_value_ratio = (unique_counts.sum() - unique_counts.max()) / unique_counts.sum()\n",
    "        if minor_value_ratio < 0.005:\n",
    "            keep = np.argsort(preds[:, i])[::-1][:int(len(preds) * 0.005) + 1]\n",
    "            binned[keep] = preds[keep, i]\n",
    "        preds[:, i] = binned\n",
    "    return preds\n",
    "\n",
    "\n",
    "def compute_spearmanr(trues, preds, n_bins=None):\n",
    "    rhos = []\n",
    "    if n_bins:\n",
    "        preds = binning_output(preds, n_bins)\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        if len(np.unique(col_pred)) == 1:\n",
    "            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n",
    "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n",
    "    if question_only:\n",
    "        outputs = outputs[:, :21]\n",
    "        targets = targets[:, :21]\n",
    "    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n",
    "    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n",
    "    \n",
    "    batch_size = outputs.size(0)\n",
    "    if batch_size % 2 == 0:\n",
    "        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
    "        targets1, targets2 = targets.contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
    "        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n",
    "        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n",
    "        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n",
    "        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n",
    "    else:\n",
    "        # batch size is not even number, so we can't devide them into pairs.\n",
    "        margin_rank_loss = 0.0\n",
    "\n",
    "    return alpha * bce + (1 - alpha) * margin_rank_loss\n",
    "\n",
    "\n",
    "def train_and_predict(train_data, valid_data, test_data, q_train_data, q_valid_data, q_test_data, q_epochs, epochs, batch_size, fold):\n",
    "    dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    q_dataloader = torch.utils.data.DataLoader(q_train_data, shuffle=True, batch_size=batch_size)\n",
    "    q_valid_dataloader = torch.utils.data.DataLoader(q_valid_data, shuffle=False, batch_size=batch_size)\n",
    "    q_test_dataloader = torch.utils.data.DataLoader(q_test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    model = Model().to(device)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n",
    "            \"weight_decay\": 1e-2,\n",
    "            \"lr\": 5e-5\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": 5e-5\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n",
    "            \"weight_decay\": 1e-2,\n",
    "            \"lr\": 5e-4\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(len(dataloader) * (q_epochs) * 0.05),\n",
    "        num_training_steps=len(dataloader) * (q_epochs)\n",
    "    )\n",
    "    \n",
    "    test_predictions = []\n",
    "    valid_predictions = []\n",
    "\n",
    "    ## Question Only\n",
    "    for epoch in range(q_epochs): \n",
    "        import time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        for input_ids, token_type_ids, attention_mask, targets in tqdm(q_dataloader, total=len(q_dataloader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n",
    "            train_targets.extend(targets.detach().cpu().numpy())\n",
    "            loss = compute_loss(outputs, targets, question_only=True)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.detach().cpu().item())\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "        with torch.no_grad():\n",
    "            for input_ids, token_type_ids, attention_mask, targets in tqdm(q_valid_dataloader, total=len(q_valid_dataloader)):\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                prob = outputs.sigmoid()\n",
    "                prob[:, 21:] = 0.0\n",
    "                valid_preds.extend(prob.cpu().numpy())\n",
    "                valid_targets.extend(targets.cpu().numpy())\n",
    "                loss = compute_loss(outputs, targets, question_only=True)\n",
    "                valid_losses.append(loss.detach().cpu().item())\n",
    "            valid_predictions.append(np.stack(valid_preds))\n",
    "            test_preds = []\n",
    "            for input_ids, token_type_ids, attention_mask in tqdm(q_test_dataloader, total=len(q_test_dataloader)):\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                prob = outputs.sigmoid()\n",
    "                prob[:, 21:] = 0.0\n",
    "                test_preds.extend(prob.cpu().numpy())\n",
    "            test_predictions.append(np.stack(test_preds))\n",
    "            print()\n",
    "        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n",
    "        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n",
    "            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n",
    "            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n",
    "            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n",
    "        ))\n",
    "        print(\"\\t elapsed: {}s\".format(time.time() - start))\n",
    "\n",
    "    ## Q and A\n",
    "    model = Model().to(device)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n",
    "            \"weight_decay\": 1e-2,\n",
    "            \"lr\": 5e-5\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": 5e-5\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n",
    "            \"weight_decay\": 1e-2,\n",
    "            \"lr\": 5e-4\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n",
    "        num_training_steps=len(dataloader) * (epochs)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        import time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        for input_ids, token_type_ids, attention_mask, targets in tqdm(dataloader, total=len(dataloader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n",
    "            train_targets.extend(targets.detach().cpu().numpy())\n",
    "            loss = compute_loss(outputs, targets)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.detach().cpu().item())\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "        with torch.no_grad():\n",
    "            for input_ids, token_type_ids, attention_mask, targets in tqdm(valid_dataloader, total=len(valid_dataloader)):\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                valid_preds.extend(outputs.sigmoid().cpu().numpy())\n",
    "                valid_targets.extend(targets.cpu().numpy())\n",
    "                loss = compute_loss(outputs, targets)\n",
    "                valid_losses.append(loss.detach().cpu().item())\n",
    "            valid_predictions.append(np.stack(valid_preds))\n",
    "            test_preds = []\n",
    "            for input_ids, token_type_ids, attention_mask in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                test_preds.extend(outputs.sigmoid().cpu().numpy())\n",
    "            test_predictions.append(np.stack(test_preds))\n",
    "            print()\n",
    "        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n",
    "        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n",
    "            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n",
    "            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n",
    "            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n",
    "        ))\n",
    "        print(\"\\t elapsed: {}s\".format(time.time() - start))\n",
    "\n",
    "    return valid_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold(object):\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=71):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_groupkfold(self, train, group_name):\n",
    "        group = train[group_name]\n",
    "        unique_group = group.unique()\n",
    "\n",
    "        kf = KFold(\n",
    "            n_splits=self.n_splits,\n",
    "            shuffle=self.shuffle,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        folds_ids = []\n",
    "        for trn_group_idx, val_group_idx in kf.split(unique_group):\n",
    "            trn_group = unique_group[trn_group_idx]\n",
    "            val_group = unique_group[val_group_idx]\n",
    "            is_trn = group.isin(trn_group)\n",
    "            is_val = group.isin(val_group)\n",
    "            trn_idx = train[is_trn].index\n",
    "            val_idx = train[is_val].index\n",
    "            folds_ids.append((trn_idx, val_idx))\n",
    "\n",
    "        return folds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "6\n",
      "5\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "gkf = Fold(n_splits=3, shuffle=True, random_state=71)\n",
    "fold_ids = gkf.get_groupkfold(df_train, group_name=\"url\")\n",
    "\n",
    "for train_idx, valid_idx in fold_ids:\n",
    "    print((df_train.loc[train_idx, \"question_type_spelling\"] > 0).sum())\n",
    "    print((df_train.loc[valid_idx, \"question_type_spelling\"] > 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss 0.16246813971785798, Valid Loss 0.14927478905781696\n",
      "\t Train Spearmanr 0.2043, Valid Spearmanr (avg) 0.2838, Valid Spearmanr (last) 0.2816\n",
      "\t elapsed: 291.57769989967346s\n",
      "\n",
      "Epoch 2: Train Loss 0.1462297767459873, Valid Loss 0.1474814944925939\n",
      "\t Train Spearmanr 0.3080, Valid Spearmanr (avg) 0.2935, Valid Spearmanr (last) 0.2942\n",
      "\t elapsed: 290.5072445869446s\n",
      "\n",
      "Epoch 3: Train Loss 0.1398472028474013, Valid Loss 0.14674534547189794\n",
      "\t Train Spearmanr 0.3503, Valid Spearmanr (avg) 0.2982, Valid Spearmanr (last) 0.3017\n",
      "\t elapsed: 290.47096729278564s\n",
      "\n",
      "Epoch 1: Train Loss 0.17775589583944235, Valid Loss 0.1660405881557947\n",
      "\t Train Spearmanr 0.2690, Valid Spearmanr (avg) 0.4037, Valid Spearmanr (last) 0.3824\n",
      "\t elapsed: 290.9947626590729s\n",
      "\n",
      "Epoch 2: Train Loss 0.1627598503634097, Valid Loss 0.16378141110509287\n",
      "\t Train Spearmanr 0.4141, Valid Spearmanr (avg) 0.4116, Valid Spearmanr (last) 0.3984\n",
      "\t elapsed: 290.91580867767334s\n",
      "\n",
      "Epoch 3: Train Loss 0.15464559506388412, Valid Loss 0.16395867439095613\n",
      "\t Train Spearmanr 0.5017, Valid Spearmanr (avg) 0.4135, Valid Spearmanr (last) 0.4013\n",
      "\t elapsed: 290.969064950943s\n",
      "\n",
      "Epoch 1: Train Loss 0.1616110454003016, Valid Loss 0.15076216732657802\n",
      "\t Train Spearmanr 0.2037, Valid Spearmanr (avg) 0.2960, Valid Spearmanr (last) 0.2885\n",
      "\t elapsed: 291.89614963531494s\n",
      "\n",
      "Epoch 2: Train Loss 0.1457214217478707, Valid Loss 0.14793903656362548\n",
      "\t Train Spearmanr 0.2920, Valid Spearmanr (avg) 0.2993, Valid Spearmanr (last) 0.3053\n",
      "\t elapsed: 291.4778516292572s\n",
      "\n",
      "Epoch 3: Train Loss 0.13876168531249966, Valid Loss 0.14757471936424887\n",
      "\t Train Spearmanr 0.3444, Valid Spearmanr (avg) 0.3062, Valid Spearmanr (last) 0.3055\n",
      "\t elapsed: 291.594997882843s\n",
      "\n",
      "Epoch 1: Train Loss 0.17738412205989545, Valid Loss 0.16737025588985502\n",
      "\t Train Spearmanr 0.2634, Valid Spearmanr (avg) 0.4091, Valid Spearmanr (last) 0.3794\n",
      "\t elapsed: 291.3872139453888s\n",
      "\n",
      "Epoch 2: Train Loss 0.16245042334294177, Valid Loss 0.1644452908553007\n",
      "\t Train Spearmanr 0.4082, Valid Spearmanr (avg) 0.4203, Valid Spearmanr (last) 0.4080\n",
      "\t elapsed: 291.5525629520416s\n",
      "\n",
      "Epoch 3: Train Loss 0.15443944822284128, Valid Loss 0.16443061549949833\n",
      "\t Train Spearmanr 0.4971, Valid Spearmanr (avg) 0.4219, Valid Spearmanr (last) 0.4094\n",
      "\t elapsed: 291.29814863204956s\n",
      "\n",
      "Epoch 1: Train Loss 0.16094084922589508, Valid Loss 0.15085830307006837\n",
      "\t Train Spearmanr 0.2046, Valid Spearmanr (avg) 0.2807, Valid Spearmanr (last) 0.2771\n",
      "\t elapsed: 292.39806842803955s\n",
      "\n",
      "Epoch 2: Train Loss 0.14522480685629097, Valid Loss 0.149187433719635\n",
      "\t Train Spearmanr 0.2994, Valid Spearmanr (avg) 0.2860, Valid Spearmanr (last) 0.2848\n",
      "\t elapsed: 292.2707748413086s\n",
      "\n",
      "Epoch 3: Train Loss 0.13882239726828594, Valid Loss 0.1479597001671791\n",
      "\t Train Spearmanr 0.3440, Valid Spearmanr (avg) 0.2903, Valid Spearmanr (last) 0.2919\n",
      "\t elapsed: 292.4531054496765s\n",
      "\n",
      "Epoch 1: Train Loss 0.17749885826134215, Valid Loss 0.1683657795190811\n",
      "\t Train Spearmanr 0.2723, Valid Spearmanr (avg) 0.3878, Valid Spearmanr (last) 0.3609\n",
      "\t elapsed: 292.27487802505493s\n",
      "\n",
      "Epoch 2: Train Loss 0.1618503256609627, Valid Loss 0.16514489167928695\n",
      "\t Train Spearmanr 0.4158, Valid Spearmanr (avg) 0.3972, Valid Spearmanr (last) 0.3857\n",
      "\t elapsed: 292.7800769805908s\n",
      "\n",
      "Epoch 3: Train Loss 0.1543229650633008, Valid Loss 0.1650715330839157\n",
      "\t Train Spearmanr 0.4999, Valid Spearmanr (avg) 0.4010, Valid Spearmanr (last) 0.3919\n",
      "\t elapsed: 293.1420361995697s\n"
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "test_dataset = torch.utils.data.TensorDataset(*test_inputs)\n",
    "q_test_dataset = torch.utils.data.TensorDataset(*test_question_only_inputs)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    train_inputs = [inputs[i][train_idx] for i in range(3)]\n",
    "    q_train_inputs = [question_only_inputs[i][train_idx] for i in range(3)]\n",
    "    train_outputs = outputs[train_idx]\n",
    "    train_dataset = torch.utils.data.TensorDataset(*train_inputs, train_outputs)\n",
    "    q_train_dataset = torch.utils.data.TensorDataset(*q_train_inputs, train_outputs)\n",
    "\n",
    "    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
    "    q_valid_inputs = [question_only_inputs[i][valid_idx] for i in range(3)]\n",
    "    valid_outputs = outputs[valid_idx]\n",
    "    valid_dataset = torch.utils.data.TensorDataset(*valid_inputs, valid_outputs)\n",
    "    q_valid_dataset = torch.utils.data.TensorDataset(*q_valid_inputs, valid_outputs)\n",
    "\n",
    "    history = train_and_predict(\n",
    "        train_data=train_dataset, \n",
    "        valid_data=valid_dataset,\n",
    "        test_data=test_dataset, \n",
    "        q_train_data=q_train_dataset, \n",
    "        q_valid_data=q_valid_dataset,\n",
    "        q_test_data=q_test_dataset, \n",
    "        q_epochs=3, epochs=3, batch_size=8, fold=fold\n",
    "        )\n",
    "\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get val preds per each epochs\n",
    "val_preds_list = []\n",
    "n_epochs = len(histories[0][0])\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    val_preds_one_epoch = np.zeros([len(df_train), 30])    \n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n",
    "        val_pred = histories[fold][0][epoch]\n",
    "        val_preds_one_epoch[valid_idx, :] += val_pred\n",
    "\n",
    "    val_preds_list.append(val_preds_one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6079, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_predictions = np.zeros((n_epochs, len(df_train), len(output_categories)), dtype=np.float32)\n",
    "\n",
    "for j, name in enumerate(output_categories):\n",
    "    for epoch in range(n_epochs):\n",
    "        col = \"{}_{}\".format(epoch, name)\n",
    "        oof_predictions[epoch, :, j] = val_preds_list[epoch][:, j]\n",
    "\n",
    "oof_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test preds per each epochs\n",
    "test_preds_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    test_preds_one_epoch = 0\n",
    "\n",
    "    for fold in range(len(fold_ids)):\n",
    "        test_preds = histories[fold][1][epoch]\n",
    "        test_preds_one_epoch += test_preds\n",
    "\n",
    "    test_preds_one_epoch = test_preds_one_epoch / len(fold_ids)\n",
    "    test_preds_list.append(test_preds_one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 476, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = np.zeros((n_epochs, len(df_test), len(output_categories)), dtype=np.float32)\n",
    "\n",
    "for j, name in enumerate(output_categories):\n",
    "    for epoch in range(n_epochs):\n",
    "        col = \"{}_{}\".format(epoch, name)\n",
    "        test_predictions[epoch, :, j] = test_preds_list[epoch][:, j]\n",
    "\n",
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import abstractmethod\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class Base_Model(object):\n",
    "    @abstractmethod\n",
    "    def fit(self, x_train, y_train, x_valid, y_valid, config):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_best_iteration(self, model):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, model, features):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_feature_importance(self, model):\n",
    "        raise NotImplementedError      \n",
    "        \n",
    "\n",
    "    def cv(self, y_train, train_features, test_features, feature_name, folds_ids, config):\n",
    "        # initialize\n",
    "        test_preds = np.zeros(len(test_features))\n",
    "        oof_preds = np.zeros(len(train_features))\n",
    "        importances = pd.DataFrame(index=feature_name)\n",
    "        best_iteration = 0\n",
    "        cv_score_list = []\n",
    "        models = []\n",
    "\n",
    "        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n",
    "            # get train data and valid data\n",
    "            x_trn = train_features.iloc[trn_idx]\n",
    "            y_trn = y_train[trn_idx]\n",
    "            x_val = train_features.iloc[val_idx]\n",
    "            y_val = y_train[val_idx]\n",
    "            \n",
    "            # train model\n",
    "            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n",
    "            cv_score_list.append(best_score)\n",
    "            models.append(model)\n",
    "            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n",
    "    \n",
    "            # predict out-of-fold and test\n",
    "            oof_preds[val_idx] = self.predict(model, x_val)\n",
    "            test_preds += self.predict(model, test_features) / len(folds_ids)\n",
    "\n",
    "            # get feature importances\n",
    "            importances_tmp = pd.DataFrame(\n",
    "                self.get_feature_importance(model),\n",
    "                columns=[f'gain_{i_fold+1}'],\n",
    "                index=feature_name\n",
    "            )\n",
    "            importances = importances.join(importances_tmp, how='inner')\n",
    "\n",
    "        # summary of feature importance\n",
    "        feature_importance = importances.mean(axis=1)\n",
    "\n",
    "        # full train\n",
    "        # model, best_score = self.full_train(train_features, y_train, config, best_iteration * 1.5)\n",
    "        # oof_preds = self.predict(model, train_features)\n",
    "        # test_preds = self.predict(model, test_features)\n",
    "    \n",
    "        evals_results = {\"evals_result\": {\n",
    "            \"cv_score\": {f\"cv{i+1}\": cv_score for i, cv_score in enumerate(cv_score_list)},\n",
    "            \"n_data\": len(train_features),\n",
    "            \"best_iteration\": best_iteration,\n",
    "            \"n_features\": len(train_features.columns),\n",
    "            \"feature_importance\": feature_importance.sort_values(ascending=False).to_dict()\n",
    "        }}\n",
    "\n",
    "        return models, oof_preds, test_preds, feature_importance, evals_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_compute_spearmanr(preds, trues):\n",
    "    rhos = spearmanr(trues.get_label(), preds).correlation\n",
    "    return \"spearmanr\", rhos, True\n",
    "\n",
    "\n",
    "def compute_spearmanr_each_col(trues, preds, n_bins=None):\n",
    "    if n_bins:\n",
    "        preds = binning_output(preds, n_bins)\n",
    "    rhos = spearmanr(trues, preds).correlation\n",
    "    return rhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class LightGBM(Base_Model):\n",
    "    def fit(self, x_train, y_train, x_valid, y_valid, config):\n",
    "        d_train = lgb.Dataset(x_train, label=y_train)\n",
    "        d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "        lgb_model_params = config[\"model\"][\"model_params\"]\n",
    "        lgb_train_params = config[\"model\"][\"train_params\"]\n",
    "        model = lgb.train(\n",
    "            params=lgb_model_params,\n",
    "            train_set=d_train,\n",
    "            valid_sets=[d_valid],\n",
    "            valid_names=['valid'],\n",
    "            feval=lgb_compute_spearmanr,\n",
    "            **lgb_train_params\n",
    "        )\n",
    "        best_score = dict(model.best_score)\n",
    "        return model, best_score\n",
    "\n",
    "    def full_train(self, x_train, y_train, config, iteration):\n",
    "        d_train = lgb.Dataset(x_train, label=y_train)\n",
    "        lgb_model_params = config[\"model\"][\"model_params\"]\n",
    "        model = lgb.train(\n",
    "            params=lgb_model_params,\n",
    "            train_set=d_train,\n",
    "            feval=lgb_compute_spearmanr,\n",
    "            num_boost_round=int(iteration)\n",
    "        )\n",
    "        best_score = dict(model.best_score)\n",
    "        return model, best_score\n",
    "\n",
    "    def get_best_iteration(self, model):\n",
    "        return model.best_iteration\n",
    "    \n",
    "    def predict(self, model, features):\n",
    "        return model.predict(features)\n",
    "        \n",
    "    def get_feature_importance(self, model):\n",
    "        return model.feature_importance(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's rmse: 0.123569\tvalid's spearmanr: 0.361336\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's rmse: 0.124009\tvalid's spearmanr: 0.410875\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.124894\tvalid's spearmanr: 0.395157\n",
      "0 question_asker_intent_understanding\n",
      "0.3788185409146311\n",
      "6079 1388\n",
      "476 379\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's rmse: 0.17538\tvalid's spearmanr: 0.640887\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's rmse: 0.168018\tvalid's spearmanr: 0.650678\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid's rmse: 0.171639\tvalid's spearmanr: 0.613979\n",
      "1 question_body_critical\n",
      "0.623637700859402\n",
      "6079 803\n",
      "476 281\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's rmse: 0.164423\tvalid's spearmanr: 0.515272\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's rmse: 0.18128\tvalid's spearmanr: 0.506473\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.179415\tvalid's spearmanr: 0.430969\n",
      "2 question_conversational\n",
      "0.34037797044021834\n",
      "6079 9\n",
      "476 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's rmse: 0.335798\tvalid's spearmanr: 0.323402\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's rmse: 0.331073\tvalid's spearmanr: 0.309812\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.336533\tvalid's spearmanr: 0.297151\n",
      "3 question_expect_short_answer\n",
      "0.28901593908277334\n",
      "6079 1140\n",
      "476 377\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.272666\tvalid's spearmanr: 0.381402\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's rmse: 0.278672\tvalid's spearmanr: 0.361174\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's rmse: 0.28372\tvalid's spearmanr: 0.316971\n",
      "4 question_fact_seeking\n",
      "0.2869148478852155\n",
      "6079 333\n",
      "476 95\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's rmse: 0.301033\tvalid's spearmanr: 0.47124\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's rmse: 0.31217\tvalid's spearmanr: 0.514642\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's rmse: 0.300469\tvalid's spearmanr: 0.478462\n",
      "5 question_has_commonly_accepted_answer\n",
      "0.37417164065294106\n",
      "6079 69\n",
      "476 34\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's rmse: 0.126439\tvalid's spearmanr: 0.357461\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's rmse: 0.129582\tvalid's spearmanr: 0.406446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's rmse: 0.124676\tvalid's spearmanr: 0.337522\n",
      "6 question_interestingness_others\n",
      "0.3582520438037975\n",
      "6079 1393\n",
      "476 282\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.161094\tvalid's spearmanr: 0.503469\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.168139\tvalid's spearmanr: 0.543715\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's rmse: 0.156811\tvalid's spearmanr: 0.507984\n",
      "7 question_interestingness_self\n",
      "0.4964401695432705\n",
      "6079 509\n",
      "476 157\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.26882\tvalid's spearmanr: 0.594103\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.281912\tvalid's spearmanr: 0.57875\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's rmse: 0.272871\tvalid's spearmanr: 0.59412\n",
      "8 question_multi_intent\n",
      "0.5587606910797699\n",
      "6079 157\n",
      "476 98\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's rmse: 0.0418115\tvalid's spearmanr: 0.113986\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's rmse: 0.0520726\tvalid's spearmanr: 0.136895\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's rmse: 0.0421254\tvalid's spearmanr: 0.109451\n",
      "9 question_not_really_a_question\n",
      "0.07173516110414939\n",
      "6079 64\n",
      "476 37\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's rmse: 0.324748\tvalid's spearmanr: 0.447256\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's rmse: 0.315002\tvalid's spearmanr: 0.501955\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's rmse: 0.325651\tvalid's spearmanr: 0.422333\n",
      "10 question_opinion_seeking\n",
      "0.45306637905930064\n",
      "6079 1811\n",
      "476 426\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's rmse: 0.26316\tvalid's spearmanr: 0.75878\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's rmse: 0.247685\tvalid's spearmanr: 0.766226\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.27684\tvalid's spearmanr: 0.727358\n",
      "11 question_type_choice\n",
      "0.7063171682171728\n",
      "6079 349\n",
      "476 162\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's rmse: 0.127123\tvalid's spearmanr: 0.5064\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.130685\tvalid's spearmanr: 0.530761\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's rmse: 0.135368\tvalid's spearmanr: 0.462605\n",
      "12 question_type_compare\n",
      "0.2697147758800353\n",
      "6079 52\n",
      "476 21\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's rmse: 0.0758723\tvalid's spearmanr: 0.35075\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.0745173\tvalid's spearmanr: 0.25501\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's rmse: 0.0617283\tvalid's spearmanr: 0.290943\n",
      "13 question_type_consequence\n",
      "0.1545999122125165\n",
      "6079 24\n",
      "476 17\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.12374\tvalid's spearmanr: 0.600495\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's rmse: 0.125967\tvalid's spearmanr: 0.650698\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's rmse: 0.132059\tvalid's spearmanr: 0.626624\n",
      "14 question_type_definition\n",
      "0.253449314733471\n",
      "6079 11\n",
      "476 6\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's rmse: 0.172668\tvalid's spearmanr: 0.594623\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's rmse: 0.168347\tvalid's spearmanr: 0.614989\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's rmse: 0.169275\tvalid's spearmanr: 0.556591\n",
      "15 question_type_entity\n",
      "0.3437016220289927\n",
      "6079 22\n",
      "476 13\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.270041\tvalid's spearmanr: 0.790301\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.259731\tvalid's spearmanr: 0.788721\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.273782\tvalid's spearmanr: 0.787521\n",
      "16 question_type_instructions\n",
      "0.7661254586940125\n",
      "6079 266\n",
      "476 141\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's rmse: 0.242766\tvalid's spearmanr: 0.327236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's rmse: 0.237184\tvalid's spearmanr: 0.357797\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's rmse: 0.25931\tvalid's spearmanr: 0.366403\n",
      "17 question_type_procedure\n",
      "0.31066511377998646\n",
      "6079 715\n",
      "476 214\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's rmse: 0.276845\tvalid's spearmanr: 0.68893\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's rmse: 0.274572\tvalid's spearmanr: 0.679336\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.29078\tvalid's spearmanr: 0.656858\n",
      "18 question_type_reason_explanation\n",
      "0.6613212281944673\n",
      "6079 645\n",
      "476 268\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.0204935\tvalid's spearmanr: 0.631451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.0245345\tvalid's spearmanr: nan\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's rmse: 0.0147052\tvalid's spearmanr: 0.315515\n",
      "19 question_type_spelling\n",
      "0.011729285466170442\n",
      "6079 5\n",
      "476 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's rmse: 0.149061\tvalid's spearmanr: 0.527652\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's rmse: 0.158684\tvalid's spearmanr: 0.530721\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's rmse: 0.153778\tvalid's spearmanr: 0.505105\n",
      "20 question_well_written\n",
      "0.5021539539601263\n",
      "6079 1578\n",
      "476 409\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.107337\tvalid's spearmanr: 0.232168\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's rmse: 0.114869\tvalid's spearmanr: 0.259303\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's rmse: 0.114173\tvalid's spearmanr: 0.217807\n",
      "21 answer_helpful\n",
      "0.20143567752649033\n",
      "6079 249\n",
      "476 123\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's rmse: 0.0954499\tvalid's spearmanr: 0.435658\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's rmse: 0.0996604\tvalid's spearmanr: 0.424891\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's rmse: 0.0989122\tvalid's spearmanr: 0.418474\n",
      "22 answer_level_of_information\n",
      "0.4210362665604529\n",
      "6079 1523\n",
      "476 325\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's rmse: 0.0835113\tvalid's spearmanr: 0.165617\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's rmse: 0.0890101\tvalid's spearmanr: 0.171764\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's rmse: 0.0852558\tvalid's spearmanr: 0.15834\n",
      "23 answer_plausible\n",
      "0.15014360846503344\n",
      "6079 623\n",
      "476 253\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's rmse: 0.0717831\tvalid's spearmanr: 0.156246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's rmse: 0.0768772\tvalid's spearmanr: 0.205618\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's rmse: 0.0703192\tvalid's spearmanr: 0.203826\n",
      "24 answer_relevance\n",
      "0.15482124570312242\n",
      "6079 795\n",
      "476 282\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid's rmse: 0.120351\tvalid's spearmanr: 0.302995\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's rmse: 0.123445\tvalid's spearmanr: 0.362735\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's rmse: 0.122979\tvalid's spearmanr: 0.303881\n",
      "25 answer_satisfaction\n",
      "0.31484470580657675\n",
      "6079 1404\n",
      "476 369\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's rmse: 0.28634\tvalid's spearmanr: 0.767087\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's rmse: 0.276442\tvalid's spearmanr: 0.750645\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's rmse: 0.291832\tvalid's spearmanr: 0.749293\n",
      "26 answer_type_instructions\n",
      "0.733847351339801\n",
      "6079 344\n",
      "476 157\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's rmse: 0.219397\tvalid's spearmanr: 0.277596\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid's rmse: 0.21761\tvalid's spearmanr: 0.279119\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's rmse: 0.219819\tvalid's spearmanr: 0.284274\n",
      "27 answer_type_procedure\n",
      "0.26365252577150666\n",
      "6079 2208\n",
      "476 476\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's rmse: 0.304995\tvalid's spearmanr: 0.688575\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's rmse: 0.302652\tvalid's spearmanr: 0.67237\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's rmse: 0.305752\tvalid's spearmanr: 0.656491\n",
      "28 answer_type_reason_explanation\n",
      "0.6665974013791681\n",
      "6079 1427\n",
      "476 391\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's rmse: 0.0991144\tvalid's spearmanr: 0.23746\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's rmse: 0.099715\tvalid's spearmanr: 0.202552\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's rmse: 0.0952035\tvalid's spearmanr: 0.21464\n",
      "29 answer_well_written\n",
      "0.20623576725419485\n",
      "6079 226\n",
      "476 129\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"name\": \"lightgbm\",\n",
    "        \"model_params\": {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"objective\": \"rmse\",\n",
    "            \"tree_learner\": \"serial\",\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 1,\n",
    "            \"seed\": 71,\n",
    "            \"bagging_seed\": 71,\n",
    "            \"feature_fraction_seed\": 71,\n",
    "            \"drop_seed\": 71,\n",
    "            \"verbose\": -1\n",
    "        },\n",
    "        \"train_params\": {\n",
    "            \"num_boost_round\": 5000,\n",
    "            \"early_stopping_rounds\": 200,\n",
    "            \"verbose_eval\": 500\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "outputs = compute_output_arrays(df_train)\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "\n",
    "for i_col in range(len(output_categories)):\n",
    "    y_train = outputs[:, i_col]\n",
    "    #x_train = pd.DataFrame(oof_predictions[:, :, 2].T)\n",
    "    x_train = pd.DataFrame(np.concatenate([oof_predictions[:, :, i].T for i in range(30)], axis=1))\n",
    "    x_test = pd.DataFrame(np.concatenate([test_predictions[:, :, i].T for i in range(30)], axis=1))\n",
    "    feature_name = x_train.columns\n",
    "\n",
    "    model = LightGBM()\n",
    "    models, oof_preds, test_preds, feature_importance, evals_results = model.cv(\n",
    "            y_train, x_train, x_test, feature_name, fold_ids, config\n",
    "    )\n",
    "    oof_preds_list.append(oof_preds.reshape(-1, 1))\n",
    "    test_preds_list.append(test_preds.reshape(-1, 1))\n",
    "\n",
    "    print(i_col, output_categories[i_col])\n",
    "    print(compute_spearmanr_each_col(oof_preds, y_train))\n",
    "    print(len(oof_preds), len(np.unique(oof_preds)))\n",
    "    print(len(test_preds), len(np.unique(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37745278224662554\n"
     ]
    }
   ],
   "source": [
    "def compute_spearmanr(trues, preds, n_bins=None):\n",
    "    rhos = []\n",
    "    if n_bins:\n",
    "        preds = binning_output(preds, n_bins)\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        if len(np.unique(col_pred)) == 1:\n",
    "            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n",
    "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "\n",
    "oof_preds_fi = np.concatenate(oof_preds_list, axis=1)\n",
    "print(compute_spearmanr(outputs, oof_preds_fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_fi = np.concatenate(test_preds_list, axis=1)\n",
    "sub.iloc[:, 1:] = test_preds_fi\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
